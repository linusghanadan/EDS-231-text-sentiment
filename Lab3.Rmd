---
title: "Lab3"
author: "Linus Ghanadan"
date: "2024-04-17"
output: html_document
---
### Assignment Lab 3:

Due next week: April 23 at 11:59PM

For this assignment you'll use the article data you downloaded from Nexis Uni in Week 2.

```{r}
# load libraries
library(LexisNexisTools)
library(quanteda)
library(tm)
library(topicmodels)
library(ldatuning)
library(tidyverse)
library(tidytext)
library(reshape2)
library(LDAvis) # visualization
library(tsne) # matrix decomposition
```

1.  Create a corpus from your articles.


```{r}
# set the working directory
setwd(here("nexis"))

# list all .docx files from directory
plastic_files <- list.files(pattern = ".docx", path = getwd(),
                            full.names = TRUE, recursive = TRUE, ignore.case = TRUE)

# read files
dat <- lnt_read(plastic_files)
```

```{r}

# extract metadata, article text, and paragraph text
meta_df <- dat@meta
articles_df <- dat@articles
paragraphs_df <- dat@paragraphs

# create tidy data frame
dat2 <- tibble(Date=meta_df$Date, Headline = meta_df$Headline, id = articles_df$ID, text = articles_df$Article)

# create corpus text from df
dat2_corpus <- corpus(dat2$text)
```


2.  Clean the data as appropriate.

```{r}
# add stop words
add_stops <- stopwords(kind = quanteda_options("langauge_stopwords"))

# examine tokens
# tokens(dat2_corpus)
```

```{r}
# remove punctuation, numbers, and URL
toks <- tokens(dat2_corpus, remove_punct = T, remove_numbers = T, remove_url = T)

# remove stop words
tok1 <- tokens_select(toks, pattern = add_stops, selection = "remove")

# convert to lower case
dfm1 <- dfm(tok1, tolower = T)

# remove words that only show up 1 or 2 times
dfm2 <- dfm_trim(dfm1, min_docfreq = 2)

# compute row sums and subset to only include rows with a sum > 0
sel_idx <- slam::row_sums(dfm2) > 0
dfm <- dfm2[sel_idx, ]

```


3.  Run three models (i.e. with 3 values of k) and select the overall best value for k (the number of topics) - include some justification for your selection: theory, FindTopicsNumber() optimization metrics, interpretability, LDAvis. Select the best single value of k.

```{r}
# run three models
results <- FindTopicsNumber(dfm,
                            topics = seq(from = 2,
                                         to = 20,
                                         by = 1),
                            metrics = c("CaoJuan2009", "Deveaud2014"),
                            method = "Gibbs",
                            verbose = T)

# plot results to select k
FindTopicsNumber_plot(results)
```

In trying to strike a balance between a high value for Deveaud2014 and a low value for CaoJuan2009, I am selecting k=5 because has a very high value for Deveaud2014 that is above 0.90 and a relatively low value for CaoJuan2009 of about 0.30. It would be better if I could have had a lower value for CaoJuan2009, but given the distribution in Deveaud2014 and CaoJuan2009 across different numbers of topics, 5 seems like the best choice in this situation, especially given that I do not want the burden of interpreting a large number of different topics.

```{r}
# set value for k 
k <- 5

# run model
topicModel_k5 <- LDA(dfm,
                     k,
                     method = "Gibbs",
                     control = list(iter = 1000),
                     verbose = 25
)

# get results
results <- posterior(topicModel_k5)
attributes(results)

# define matrices for interpretation
beta <- results$terms
theta <- results$topics

# tidy matrices
topics <- tidy(topicModel_k5, matrix = "beta")

# look at top terms
top_terms <- topics %>% 
  group_by(topic) %>% 
  top_n(10, beta) %>% 
  ungroup() %>% 
  arrange(topic, -beta)
top_terms
```

4.  Plot the top terms in each topic and the distribution of topics across a sample of the documents (constrained by what looks good in the plot).

```{r}
# create plot
top_terms %>%
  mutate(term = reorder_within(term, beta, topic, sep = "")) %>% 
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = F) +
  facet_wrap(~topic, scales = "free_y") +
  scale_x_reordered() +
  coord_flip()
```

```{r}
# assign names to topics
topic_words <- terms(topicModel_k5, 5)
topic_names <- apply(topic_words, 2, paste, collapse = " ")

# define examples
example_ids <- c(1:5)
n <- length(example_ids)

# obtain topic proportions for example documents
example_props <- theta[example_ids,]
colnames(example_props) <- topic_names

# prepare data for plotting
viz_df <- melt(cbind(data.frame(example_props),
                     document = factor(1:n),
                     variable.name = "topic",
                     id.vars = "document"))

# create plot
ggplot(data = viz_df, aes(variable, value, fill = document),
       ylab = "proportion") +
  geom_bar(stat = "identity") +
  coord_flip() + 
  facet_wrap(~ document, ncol = n)
```

```{r}
# define function that does SVD and then performs t-SNE on resulting U matrix 
svd_tsne <- function(x) tsne(svd(x)$u)

# create JSON object
json <- createJSON(
  phi = beta,
  theta = theta,
  doc.length = rowSums(dfm),
  vocab = colnames(dfm),
  term.frequency = colSums(dfm),
  mds.method = svd_tsne,
  plot.opts = list(xlab="", ylab="")
)

# visualize JSON object
serVis(json)
```

5.  Take a stab at interpreting the resulting topics. What are the key themes discussed in the articles in your data base?

Some key themes discussed in the articles that I have analyzed are the use of plastic in bags and packaging, specifically single-use packaging. These specific instances of where plastic is used, alongside words such as "ban", "tax", and "law", suggest that these sorts of items (plastic bags and plastic packaging) are subjected to regulations in certain areas. In addition, the high frequency of the words "UK", "Canada", and "CEPA" (Center for European Policy Analysis) suggests that the U areas of the world are the ones where these sort of regulatory dynamics are either being proposed or have been implemented.

Interestingly, in cluster 5, where nearly all mentions of "UK" exist, is also the same cluster where nearly all mentions of "tax" exist, indicating that the UK stands out as an area that is either proposing or implementing a tax regarding plastic production or consumption.

Furthermore, in cluster 1, where nearly all mentions of "Canada" exist, is also the same cluster where nearly all mentions of "proposed", "plan", and "paper" exist, indicating that Canada might be in the process of proposing regulations that will cover both plastic and paper materials.
